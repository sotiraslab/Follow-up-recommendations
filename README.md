# Large-scale evaluation of machine learning models in identifying follow-up recommendations in radiology reports

This is the official code repository for the manuscript "Large-scale evaluation of machine learning models in identifying follow-up recommendations in radiology reports".

![alt text](./figures/Figure_1.jpg)
**Study flowchart.** *(A) Three sets of radiology reports were gathered: the first one extracted through two regular expressions (Section S1), the second was annotated during dictation, while the third was annotated by three radiologists. The training set was created by combining the dataset extracted using regular expressions with 20% of reports randomly selected from the two manually annotated corpora. The remaining 80% of radiologist-annotated reports was divided into validation and test sets with a ratio of 1:4. (B) Reports lacking a finding section from the training, validation, and test sets were excluded from the analysis, as this section was missing. (C) Any reports from the training, validation, and test sets that did not include an impression section were eliminated from consideration. (D) The impression sections of 2,000 MIMIC-CXR reports were collected as an external test set. (E) The impression sections 100 CT institutional reports were gathered at a different time point to serve as a temporal test set.*